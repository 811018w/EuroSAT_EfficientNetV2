{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import argparse, random, numpy as np, torch, gc\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import EuroSAT\n",
        "from tqdm import tqdm\n",
        "from contextlib import nullcontext\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "zBsrvnI3rGjr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 1. 參數 ----------\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data_dir', type=str, default='./data')\n",
        "parser.add_argument('--batch_size', type=int, default=64)\n",
        "parser.add_argument('--epochs', type=int, default=64)\n",
        "parser.add_argument('--lr', type=float, default=5e-4)\n",
        "parser.add_argument('--accum_steps', type=int, default=1,\n",
        "                    help='梯度累積步數')\n",
        "parser.add_argument('--variant', '-v', choices=['s', 'm', 'l'], default='m')\n",
        "parser.add_argument('--freeze_backbone', action='store_true')\n",
        "args, _ = parser.parse_known_args()\n",
        "\n",
        "# ---------- 2. 環境 ----------\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(42); random.seed(42); np.random.seed(42)\n",
        "\n",
        "try:\n",
        "    from torch.amp import autocast as _ac\n",
        "    def amp_ctx(): return _ac(device_type=DEVICE) if DEVICE == 'cuda' else nullcontext()\n",
        "except ImportError:\n",
        "    from torch.cuda.amp import autocast as _ac\n",
        "    def amp_ctx(): return _ac() if DEVICE == 'cuda' else nullcontext()\n",
        "\n",
        "from torch.cuda.amp import GradScaler\n",
        "scaler = GradScaler(enabled=(DEVICE == 'cuda'))\n",
        "\n",
        "# ---------- 3. 資料 ----------\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize(224, antialias=True),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "dataset = EuroSAT(args.data_dir, download=True, transform=tfm)\n",
        "n_total = len(dataset)\n",
        "n_val = int(n_total * 0.15)  # 確保是整數\n",
        "n_test = int(n_total * 0.15)  # 確保是整數\n",
        "n_train = n_total - n_val - n_test\n",
        "\n",
        "train_set, val_set, test_set = random_split(\n",
        "    dataset, [n_train, n_val, n_test],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "\n",
        "def make_loader(split, bs):\n",
        "    return DataLoader(split, batch_size=bs,\n",
        "                      shuffle=(split is train_set),\n",
        "                      num_workers=2, pin_memory=True)\n",
        "\n",
        "train_loader = make_loader(train_set, args.batch_size)\n",
        "val_loader   = make_loader(val_set,   args.batch_size)\n",
        "test_loader  = make_loader(test_set,  args.batch_size)\n",
        "num_classes = len(dataset.classes)"
      ],
      "metadata": {
        "id": "HiTbdxWwZcwq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da2ddb39-995f-42ed-bf8c-894f8f5ed6a4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-e8becc1ec16e>:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=(DEVICE == 'cuda'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 4. 模型 ----------\n",
        "v = args.variant\n",
        "if v == 's':\n",
        "    net = models.efficientnet_v2_s(\n",
        "        weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "elif v == 'm':\n",
        "    net = models.efficientnet_v2_m(\n",
        "        weights=models.EfficientNet_V2_M_Weights.IMAGENET1K_V1)\n",
        "else:\n",
        "    net = models.efficientnet_v2_l(\n",
        "        weights=models.EfficientNet_V2_L_Weights.IMAGENET1K_V1)\n",
        "\n",
        "net.classifier[1] = nn.Linear(net.classifier[1].in_features, num_classes)\n",
        "\n",
        "if args.freeze_backbone:\n",
        "    for n, p in net.named_parameters():\n",
        "        if not n.startswith('classifier'):\n",
        "            p.requires_grad = False\n",
        "    print('Backbone frozen；僅微調分類頭')\n",
        "\n",
        "net = net.to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "opt = optim.AdamW([p for p in net.parameters() if p.requires_grad],\n",
        "                  lr=args.lr, weight_decay=1e-4)\n",
        "sched = ReduceLROnPlateau(opt, 'min', patience=2, factor=0.5, verbose=True)"
      ],
      "metadata": {
        "id": "wT2airTakvj3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11974f52-60d6-4034-98b4-4781a453b3aa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_v2_m-dc08266a.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_m-dc08266a.pth\n",
            "100%|██████████| 208M/208M [00:01<00:00, 190MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 5. 訓練 ----------\n",
        "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "\n",
        "def forward_step(x, y):\n",
        "    with amp_ctx():\n",
        "        pred = net(x)\n",
        "        loss = criterion(pred, y)\n",
        "    return loss, pred\n",
        "\n",
        "best_val = 0; stale = 0\n",
        "for ep in range(1, args.epochs + 1):\n",
        "    net.train()\n",
        "    tloss = tcorrect = ttotal = 0\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "\n",
        "    for idx, (x, y) in enumerate(tqdm(train_loader, desc=f'Epoch {ep}/{args.epochs}')):\n",
        "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
        "        try:\n",
        "            loss, pred = forward_step(x, y)\n",
        "        except RuntimeError as e:\n",
        "            if 'out of memory' in str(e):\n",
        "                print('CUDA OOM，調小 --batch_size 或加大 --accum_steps')\n",
        "                torch.cuda.empty_cache(); gc.collect()\n",
        "                raise e\n",
        "        scaler.scale(loss / args.accum_steps).backward()\n",
        "        tcorrect += (pred.argmax(1) == y).sum().item()\n",
        "        ttotal   += y.size(0)\n",
        "        tloss    += loss.item() * y.size(0)\n",
        "\n",
        "        if (idx + 1) % args.accum_steps == 0:\n",
        "            scaler.step(opt); scaler.update()\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "    train_loss = tloss / ttotal\n",
        "    train_acc  = tcorrect / ttotal\n",
        "\n",
        "    # ----- 驗證 -----\n",
        "    net.eval()\n",
        "    vloss = vcorrect = vtotal = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
        "            loss, pred = forward_step(x, y)\n",
        "            vloss    += loss.item() * y.size(0)\n",
        "            vcorrect += (pred.argmax(1) == y).sum().item()\n",
        "            vtotal   += y.size(0)\n",
        "    val_loss = vloss / vtotal\n",
        "    val_acc  = vcorrect / vtotal\n",
        "\n",
        "    # 紀錄\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    sched.step(val_loss)\n",
        "\n",
        "    if val_acc > best_val:\n",
        "        best_val, stale = val_acc, 0\n",
        "        torch.save(net.state_dict(), f'best_effv2_{v}.pt')\n",
        "    else:\n",
        "        stale += 1\n",
        "        if stale >= 5:\n",
        "            print('Early stopping'); break\n",
        "\n",
        "    print(f'Epoch {ep:02d} | '\n",
        "          f'train_loss {train_loss:.4f} train_acc {train_acc:.4f} | '\n",
        "          f'val_loss {val_loss:.4f} val_acc {val_acc:.4f}')"
      ],
      "metadata": {
        "id": "eaIU7lyYqQE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23cb2407-1d44-4531-a825-bb79524c6fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/64:  21%|██        | 62/296 [00:35<02:08,  1.82it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 6. 繪圖 ----------\n",
        "epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "# Loss 曲線\n",
        "plt.figure()\n",
        "plt.plot(epochs, history['train_loss'], label='train_loss')\n",
        "plt.plot(epochs, history['val_loss'],   label='val_loss')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss Curve'); plt.legend()\n",
        "plt.savefig('loss_curve.png', dpi=300, bbox_inches='tight')   # ← 新增\n",
        "plt.show()\n",
        "\n",
        "# Accuracy 曲線\n",
        "plt.figure()\n",
        "plt.plot(epochs, history['train_acc'], label='train_acc')\n",
        "plt.plot(epochs, history['val_acc'],   label='val_acc')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy Curve'); plt.legend()\n",
        "plt.savefig('accuracy_curve.png', dpi=300, bbox_inches='tight')  # ← 新增\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NHPwH2O-1zND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 7. 測試 ----------\n",
        "from sklearn.metrics import classification_report\n",
        "net.load_state_dict(torch.load(f'best_effv2_{v}.pt', map_location=DEVICE))\n",
        "net.eval()\n",
        "preds, labels = [], []\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        _, pred = forward_step(x, y.to(DEVICE))\n",
        "        preds.extend(pred.argmax(1).cpu().numpy())\n",
        "        labels.extend(y.numpy())\n",
        "\n",
        "print(classification_report(labels, preds, target_names=dataset.classes, digits=4))"
      ],
      "metadata": {
        "id": "OdmjaUWaqTKJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}